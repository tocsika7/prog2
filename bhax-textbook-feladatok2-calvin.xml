<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0" xml:lang="hu">
    <info>
        <title>Helló,Calvin!</title>
        <keywordset>
            <keyword/>
        </keywordset>
    </info>

    <section>
        <title>MNIST </title>
        <para>Megoldás forrása: <link xlink:href="https://gitlab.com/tocsika7/prog2_forrasok/tree/master/calvin/mnist.py">GitLab</link> </para>
		<para>Tanulságok, tapasztalatok, magyarázat: </para>
        <para>A feladat az volt, hogy próbáljuk ki az MNIST-et, illetve készítsünk hozzá egy saját
            8-ast tesztelésre. A MNIST egy olyan program ami adatbázisból való tanítás után
            felismeri a kézzel írott egyjegyű számokat.  A programohoz Python-t és az ehhez
            szükséges Tensorflow library-t használjuk. A Tensorflow az adatfolyam programozáshoz
            kínál nekünk lehetőségket. Az adatfolyam programozásban az egyes prgoramrészeket, mint
            irányított gráfok értelmezzük és ezek között pedig adatfolyamok áramlanak. </para>
        <para>A programban dekalrálunk egy függvényt a saját 8-ast ábrázoló képünk beolvasására. A
            file változóba a read_file() függgvénnyel olvassuk be a képet, majd a decode_png()
            függvénnyel, ezt dekódoljuk unit8-as vagy 16-os
            tensorrá.<programlisting>def readimg():
    file = tf.io.read_file("sajat8a.png")
    img = tf.image.decode_png(file,channels=1)
    return img</programlisting>
            A main függvényben beolvassuk az MNIST adabázis képeit majd létrehozunk egy modellt.A
            modell tensor-okból áll amik, olyan adatszerkezetek, amik (ebben az esetben) mátrixokat
            tartalmaznak. A modellben az x egy Placeholder típusú tensor, ami akkor fog értéket
            kapni, mielőtt a kód lefut. Ezután jön két változó típusú tensor, a W tartalmazza a
            súlyokat, a b pedig a bias-t, tehát a hibás következetéseket. Az y ban vesszük az x és W
            tensor mátrixszorzatát a matmul() függvénnyel, majd ehhez hozzáadjuk a b-t.
            <programlisting>mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

  # Create the model
  x = tf.placeholder(tf.float32, [None, 784])
  W = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  y = tf.matmul(x, W) + b</programlisting>
            A cross_entropy-ban vesszük a y logits-ok és y label-ök közötti keresztszorzatot, majd
            ennek a tensor-nak az elemeinek számoljuk ki a dimenziók közötti átlagát a reduce_mean()
            függvénnyel. Tanításnál ezt az átlagot próbáljuk minimalizálni a gradient descent
            algoritmus segítségével. Ez a programban a train.GradientDescentOptimizer.minimize()
            függvényként jelenik
            meg.<programlisting>  cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=y_))
  train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)</programlisting>
            Ezután elkezdjük a tanítást az MNIST képeivel egyszerre 100-at fog feldolgozni, és ezt a
            lépést ismételjük meg 10-szer, és kiírjük hány százaléknál jár a tanítás.
            <programlisting>  for i in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
    if i % 100 == 0:
      print(i/10, "%")</programlisting>Végül
            teszteljük az adatázis 42. elemére ami egy 4-es, illetve a saját kezűleg rajzolt 8-asra
            is. </para>
        <para>A progpater-es forrás működéséhez néhány függvényt át kellett írni, mert depraceted-ek
            voltak. </para>
        <para>A kép dekódolásánál meg kellett adni egy channels=1 argumentumot, ez a színcsatornák
            számát adja meg. Az érték 1 mert a kép szürkeárnyalatos.
            <programlisting>img = tf.image.decode_png(file,channels=1)</programlisting></para>
        <para>A keresztszorzat létrehozásánál pedig a softmax_cross_entropy_with_logits függvénynél
            argmuntumként meg kellett adni a logits és labels értékeket.
            <programlisting>cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y,labels=y_))</programlisting></para>
        <figure>
            <title>A program sikeresen felismeri a saját 8-ast</title>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="Images/mnist.png"/>
                </imageobject>
            </mediaobject>
        </figure>
    </section>
    <section>
        <title>Deep MNIST</title>
        <para>Megoldás forrása: <link xlink:href="https://gitlab.com/tocsika7/prog2_forrasok/blob/master/calvin/deep.py">GitLab</link> </para>
        <para>Tanulságok, tapasztalatok, magyarázat:</para>
        <para>A feladat az volt, hogy futtassuk le az MNIST mély verzióját és ismertessük fel vele a
            saját kézzel írott számunkat. </para>
        <para>A feladatban Rácz András tutoriált. </para>
        <para>Ahhoz, hogy a program az új TensorFlow-val is műkdöjön ki kell cserélnünk a legtöbb
            függvény előtagját tf.compat.v1.-re, mivel ezek az új verzióban ebbe a modulba kerültek. </para>
        <para>Ahhoz, hogy saját képet is felismerjen bele kellett írni az előző feladatban használt
            readimg() függvényt, ami be fogja olvasni a saját képunket.
            <programlisting>def readimg():
  file = tf.compat.v1.read_file("sajat8a.png")
  img = tf.image.decode_png(file, 1)
  return img</programlisting></para>
        <para>Az alábbi függvénnyel pedig a beolvasott képet fogjuk átkonvertálni 28*28-as méretűre
            és grayscale-re. Ez a paramatérekben is látszik az utolsó paraméter 1 amiért a képünk
            grayscale, ha RGB színterű lenne akkor 3-at kéne megadni.
            <programlisting>with tf.name_scope('reshape'):
    x_image = tf.reshape(x, [-1, 28, 28, 1])</programlisting></para>
        <para>Létrehozzuk a súlyokat és a biast tartalmazó Tensorokat, az első rétegben itt a
            képünkből 32 képet készít a
            program.<programlisting>  with tf.name_scope('conv1'):
    W_conv1 = weight_variable([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</programlisting></para>
        <para>A második rétegben 32 képre még helyezünk 64 filtert ami által kapunk egy 32x64 képet. </para>
        <para>Ezek után fog átmenni két teljesen kapcsolt rétegen ahol a súlyokat és biast
            beállítjuk, majd ezután történik a klasszifikáció tehát felismeri a képet. </para>
        <para>Az első feladat példájához hasonlóan is  beolvassuk a mintaképeket, majd létehozunk
            egy modellt. Létrehozunk egy keresztszorzatot a logits és labels elemek között, majd
            ezeknek az átalgát vesszük. Az optimalizálás során most az Adam algoritmust használjuk a
            GradientDescent helyett.
            <programlisting>  # Import data
  tf.compat.v1.disable_eager_execution()
  mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)

  # Create the model
  x = tf.compat.v1.placeholder(tf.float32, [None, 784])

  # Define loss and optimizer
  y_ = tf.compat.v1.placeholder(tf.float32, [None, 10])

  # Build the graph for the deep net
  y_conv, keep_prob = deepnn(x)

  with tf.name_scope('loss'):
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,
                                                            logits=y_conv)
  cross_entropy = tf.reduce_mean(cross_entropy)

  with tf.name_scope('adam_optimizer'):
    train_step = tf.compat.v1.train.AdamOptimizer(1e-4).minimize(cross_entropy)</programlisting></para>
        <para>Ezt követi a tanítás. A tanítás során 20000 mintaképet használunk és ezeket 50-essével
            olvassuk be. Illetve 100 képenként kiírjuk hány képet olvassot már be a program és hány
            százalékos a pontosság. Ez a verzió a sima MNIST-nél 20-szor több mintával dolgozik,
            ezért sokkal pontosabb de a tanítás is jóval több időt igényel. Az én esetemben ez
            körülbelül 20-25 perc volt.
            <programlisting>  with tf.compat.v1.Session() as sess:
    sess.run(tf.compat.v1.global_variables_initializer())
    for i in range(20000):
      batch = mnist.train.next_batch(50)
      if i % 100 == 0:
        train_accuracy = accuracy.eval(feed_dict={
            x: batch[0], y_: batch[1], keep_prob: 1.0})
        print('step %d, training accuracy %g' % (i, train_accuracy))
      train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

    print('test accuracy %g' % accuracy.eval(feed_dict={
        x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))</programlisting></para>
        <figure>
            <title>A program sikeresen felismerte a saját 8-ast</title>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="Images/dmnist1.png"/>
                </imageobject>
            </mediaobject>
        </figure>
        <informalfigure>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="Images/dmnist2.png"/>
                </imageobject>
            </mediaobject>
        </informalfigure>
    </section>        




	<section>
	<title>TensorFlow objektum detektáló</title>
	    <para>Megoldás forrása:  <link xlink:href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android">TensorFlow GitHub</link>
        </para>
		<para>Tanulságok, tapasztalatok, magyarázat: </para>
        <para>A feladat az volt, hogy próbáljuk a ki a TensorFlow objektum detektáló appját
            androidra. Ehhez először le kell klónozni a TensorFlow android-os repóját a következő linkről: <link
                xlink:href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android"
                >https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android</link>.
        </para>
            <para>Majd Android Studioba beimportálni és ott build-elni belőle egy .apk fájlt amit
            aztán telepíthetünk az Androidos telefonra.Ehhez a  build.gradel fájlban a
            nativeBuildSystem-et át kell írnunk 'bazel'-ről 'none'-ra. Illetve a gradle/wrapper
            mappán belül a gradle-wrapper.properties fájlban a distributionUrl értéket át kell
            írnunk egy újabb verziójúra, minimum 4.10-esre. pl.: <link
                xlink:href="https://services.gradle.org/distributions/gradle-4.10.1-all.zip"
                >https://services.gradle.org/distributions/gradle-4.10.1-all.zip</link> </para>
        <para>Ha nem szeretnénk magunknak build-elni van lehetőség már kész appot is letölteni. A
            program viszonylag jól működött állatokat, informatikai eszközöket gond nélkül
            felismert. </para>
        <para>Kipróbálva: </para>
        <informalfigure>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="Images/od_1.png"/>
                </imageobject>
            </mediaobject>
        </informalfigure>
        <informalfigure>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="Images/od_2.png"/>
                </imageobject>
            </mediaobject>
        </informalfigure>
        <informalfigure>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="Images/od_3.png"/>
                </imageobject>
            </mediaobject>
        </informalfigure>
        <informalfigure>
            <mediaobject>
                <imageobject>
                    <imagedata fileref="Images/od_4.png"/>
                </imageobject>
            </mediaobject>
        </informalfigure>
	</section>       
        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
</chapter>                
